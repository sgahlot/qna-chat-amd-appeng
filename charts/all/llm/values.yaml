global: 
  pattern: amdllm

  clusterDomain: lab.com

  amdllm:
    namespace: amd-llm
    build_envs: [] # http_proxy/https_prxy can be set here
    runtime_envs: []

    service_name: llm-tgi
    service_port: 5005
    container_port: 9000
    route_path: /v1/chat/completions
    docker_file_Path: comps/llms/src/text-generation/Dockerfile

    servingRuntime:
      name: amdserve
      namespace: amdserve

    image:
      pre_built: quay.io/sgahlot/opea/llm-tgi:latest
      from_source: image-registry.openshift-image-registry.svc:5000/opea/llm-tgi:latest

    secret_env:
      - name: HF_TOKEN
        secret:
          name: hf-token-secret
          key: huggingface
      - name: LLM_ENDPOINT
        secret:
          name: rhoai-model-secret
          key: inference_endpoint

    env:
      - name: HOME
        value: /tmp/temp-data
      - name: PYTHONPATH
        value: /home/user/.local/lib/python3.11/site-packages:/home/user
      - name: SSL_CERT_FILE
        value: /tmp/bundle.crt
      - name: REQUESTS_CA_BUNDLE
        value: /tmp/bundle.crt
      - name: LLM_MODEL_ID
        value: jary-serve
    volume:
      tmp:
        name: temp-data
        path: /tmp/temp-data
      cert:
        name: endpoint-cert
        path: /tmp
#      rhoai_ca:
#        name: rhoai-ca-bundle
#        path: /rhoai-ca
